params:
  batch_size:   110
  num_workers:  1
  max_epochs:  100
  n_buckets:   10
  seed:        42
  compile_model: false
  local: false

optimizer:
  _target_: "torch.optim.AdamW"
  lr:            1e-3      # base learning rate
  betas:         [0.9, 0.999]
  eps:           1e-8
  weight_decay: 1e-5      # decoupled L2

scheduler:
  _target_: "torch.optim.lr_scheduler.CosineAnnealingWarmRestarts"
  T_0:       ${training.params.max_epochs}   # one full cosine cycle per epoch
  T_mult:    1
  eta_min:   1e-6

early_stopping:
  patience: 10
  min_delta: 0.0

model_save_dir: "artifacts"
print_config: false

defaults:
  - loss: snr
